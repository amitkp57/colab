{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLiB_L4.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNJnC9npSKb3ZOu4odBI8e3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitkp57/colab/blob/main/MLiB_L4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwtceSK4E0gY"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIToFedE8-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a617b12-477c-406f-bae0-4e051667a628"
      },
      "source": [
        "!pip install tokenizer path tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizer in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: path in /usr/local/lib/python3.7/dist-packages (16.2.0)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 14.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh9kQrikCiPA"
      },
      "source": [
        "## Data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oph4L3CBBqsV",
        "outputId": "e14d0689-4324-402b-fd9d-74244da666e9"
      },
      "source": [
        "# import from google drive\n",
        "# run this code piece, enter the autorization code\n",
        "# For mount instructions: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# or import from environment\n",
        "PROJECT_DIR = \"/content/drive/My Drive/MLBiology/MLiB-Lab4\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlcWVsE2fXNM",
        "outputId": "e80b0190-c633-45ca-9bdd-84d88a52c1de"
      },
      "source": [
        "%cd '$PROJECT_DIR'\n",
        "!unzip '$PROJECT_DIR/MLiB-Lab4.zip' -d /content/GPT-J"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/MLBiology/MLiB-Lab4\n",
            "Archive:  /content/drive/My Drive/MLBiology/MLiB-Lab4/MLiB-Lab4.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            "   creating: /content/GPT-J/GPTJ-CP/\n",
            "   creating: /content/GPT-J/GPT-J-Run-Output/\n",
            " extracting: /content/GPT-J/GPTJ-CP/query.py  \n",
            " extracting: /content/GPT-J/install_gpt-j.sh  \n",
            " extracting: /content/GPT-J/MLiB-Lab4-GPT-Models.pdf  \n",
            " extracting: /content/GPT-J/MLiB-Lab4-GPT-Models.docx  \n",
            " extracting: /content/GPT-J/GPTJ-CP/run-query-loop.py  \n",
            " extracting: /content/GPT-J/GPTJ-CP/convert-n-run-query-loop.py  \n",
            " extracting: /content/GPT-J/GPT-J-Run-Output/drug_prompt_output.txt  \n",
            " extracting: /content/GPT-J/GPT-J-Run-Output/gene_product_output.txt  \n",
            " extracting: /content/GPT-J/GPT-J-Run-Output/drug_targets_output.txt  \n",
            " extracting: /content/GPT-J/GPT-J-Run-Output/names_patents_output.txt  \n",
            " extracting: /content/GPT-J/GPT-J-Run-Output/names_aliases_output.txt  \n",
            " extracting: /content/GPT-J/GPT-J-Run-Output/drug_inhibits_output.txt  \n",
            " extracting: /content/GPT-J/GPT-J-Run-Output/drug_mechanism_of_action_output.txt  \n",
            " extracting: /content/GPT-J/__MACOSX/._MLiB-Lab4-GPT-Models.pdf  \n",
            " extracting: /content/GPT-J/__MACOSX/._MLiB-Lab4-GPT-Models.docx  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfy5Khj1gMLW",
        "outputId": "d6310934-b339-48a0-e24f-45887656f92b"
      },
      "source": [
        "%cd /content/GPT-J\n",
        "!bash install_gpt-j.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-J\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 763.7 MB 8.5 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.9.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 2.9 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1+cu101 torchaudio-0.8.1 torchvision-0.9.1+cu101\n",
            "Cloning into 'mesh-transformer-jax'...\n",
            "remote: Enumerating objects: 769, done.\u001b[K\n",
            "remote: Total 769 (delta 0), reused 0 (delta 0), pack-reused 769\u001b[K\n",
            "Receiving objects: 100% (769/769), 247.24 KiB | 2.58 MiB/s, done.\n",
            "Resolving deltas: 100% (502/502), done.\n",
            "Collecting git+https://github.com/EleutherAI/lm-evaluation-harness/ (from -r mesh-transformer-jax/requirements.txt (line 9))\n",
            "  Cloning https://github.com/EleutherAI/lm-evaluation-harness/ to /tmp/pip-req-build-m9mek_ey\n",
            "  Running command git clone -q https://github.com/EleutherAI/lm-evaluation-harness/ /tmp/pip-req-build-m9mek_ey\n",
            "Requirement already satisfied: numpy~=1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 1)) (1.19.5)\n",
            "Collecting tqdm~=4.45.0\n",
            "  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting wandb>=0.11.2\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 10.9 MB/s \n",
            "\u001b[?25hCollecting einops~=0.3.0\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Collecting requests~=2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting fabric~=2.6.0\n",
            "  Downloading fabric-2.6.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting optax==0.0.6\n",
            "  Downloading optax-0.0.6-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting dm-haiku==0.0.5\n",
            "  Downloading dm_haiku-0.0.5-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting ray[default]==1.4.1\n",
            "  Downloading ray-1.4.1-cp37-cp37m-manylinux2014_x86_64.whl (51.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 51.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax~=0.2.12 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 11)) (0.2.25)\n",
            "Requirement already satisfied: Flask~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 12)) (1.1.4)\n",
            "Requirement already satisfied: cloudpickle~=1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 13)) (1.3.0)\n",
            "Collecting tensorflow-cpu~=2.6.0\n",
            "  Downloading tensorflow_cpu-2.6.2-cp37-cp37m-manylinux2010_x86_64.whl (172.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 172.3 MB 3.3 kB/s \n",
            "\u001b[?25hCollecting google-cloud-storage~=1.36.2\n",
            "  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart_open[gcs] in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 17)) (5.2.1)\n",
            "Collecting func_timeout\n",
            "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.70.0-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 550 kB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.15.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting lm_dataformat\n",
            "  Downloading lm_dataformat-0.0.20-py3-none-any.whl (5.8 kB)\n",
            "Collecting pathy\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt\n",
            "  Downloading https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip (16.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.5 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting black==20.8b1\n",
            "  Downloading black-20.8b1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 33.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting best_download>=0.0.6\n",
            "  Downloading best_download-0.0.7-py3-none-any.whl (4.5 kB)\n",
            "Collecting datasets==1.15.1\n",
            "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (1.0.1)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (1.8.1+cu101)\n",
            "Collecting sqlitedict==1.6.0\n",
            "  Downloading sqlitedict-1.6.0.tar.gz (29 kB)\n",
            "Collecting pytablewriter==0.58.0\n",
            "  Downloading pytablewriter-0.58.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting sacrebleu==1.5.0\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting rouge-score==0.0.4\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting pycountry==20.7.3\n",
            "  Downloading pycountry-20.7.3.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 69.2 MB/s \n",
            "\u001b[?25hCollecting numexpr==2.7.2\n",
            "  Downloading numexpr-2.7.2-cp37-cp37m-manylinux2010_x86_64.whl (471 kB)\n",
            "\u001b[K     |████████████████████████████████| 471 kB 43.7 MB/s \n",
            "\u001b[?25hCollecting pytest==6.2.3\n",
            "  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 57.1 MB/s \n",
            "\u001b[?25hCollecting pybind11==2.6.2\n",
            "  Downloading pybind11-2.6.2-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 40.4 MB/s \n",
            "\u001b[?25hCollecting tqdm-multiprocess==0.0.11\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Collecting zstandard==0.15.2\n",
            "  Downloading zstandard-0.15.2-cp37-cp37m-manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 33.2 MB/s \n",
            "\u001b[?25hCollecting jsonlines==2.0.0\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Collecting mock==4.0.3\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting openai==0.6.4\n",
            "  Downloading openai-0.6.4.tar.gz (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (0.42.1)\n",
            "Collecting nagisa==0.2.7\n",
            "  Downloading nagisa-0.2.7-cp37-cp37m-manylinux1_x86_64.whl (21.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5 MB 190 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from bleurt@ https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (2.7.0)\n",
            "Collecting tf-slim>=1.1\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.5 MB/s \n",
            "\u001b[?25hCollecting chex>=0.0.4\n",
            "  Downloading chex-0.1.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax==0.0.6->-r mesh-transformer-jax/requirements.txt (line 7)) (0.1.71+cuda111)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax==0.0.6->-r mesh-transformer-jax/requirements.txt (line 7)) (0.12.0)\n",
            "Collecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5->-r mesh-transformer-jax/requirements.txt (line 8)) (3.10.0.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5->-r mesh-transformer-jax/requirements.txt (line 8)) (0.8.9)\n",
            "Collecting ujson\n",
            "  Downloading ujson-4.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting typed-ast>=1.4.0\n",
            "  Downloading typed_ast-1.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==20.8b1->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (0.10.2)\n",
            "Collecting regex>=2020.1.8\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 31.0 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting pathspec<1,>=0.6\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==20.8b1->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (1.4.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.15.1->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (0.70.12.2)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 425 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.15.1->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (4.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.15.1->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (21.3)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.15.1->lm-eval==0.0.1->-r mesh-transformer-jax/requirements.txt (line 9)) (0.3.4)\n",
            "INFO: pip is looking at multiple versions of black to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of lm-dataformat to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of dm-haiku to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of optax to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of bleurt to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of lm-eval to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install lm-eval and tqdm~=4.45.0 because these package versions have conflicting dependencies.\u001b[0m\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested tqdm~=4.45.0\n",
            "    datasets 1.15.1 depends on tqdm>=4.62.1\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n",
            "Collecting git+https://github.com/finetuneanon/transformers@gpt-j\n",
            "  Cloning https://github.com/finetuneanon/transformers (to revision gpt-j) to /tmp/pip-req-build-b3e9ocl0\n",
            "  Running command git clone -q https://github.com/finetuneanon/transformers /tmp/pip-req-build-b3e9ocl0\n",
            "  Running command git checkout -b gpt-j --track origin/gpt-j\n",
            "  Switched to a new branch 'gpt-j'\n",
            "  Branch 'gpt-j' set up to track remote branch 'gpt-j' from 'origin'.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Collecting einops==0.3.0\n",
            "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-py3-none-any.whl size=2405406 sha256=0b920fc3a37bd66e3cefe7b3a22daba7bba9cbcbd2c79c54d9bf4286590a16f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l8vl45xk/wheels/53/9c/72/5c20589e8ea50123617a333d385c0d534a4d9b24b0c2289f48\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, einops, transformers\n",
            "Successfully installed einops-0.3.0 huggingface-hub-0.0.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.7.0.dev0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  zstd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 278 kB of archives.\n",
            "After this operation, 1,141 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 zstd amd64 1.3.3+dfsg-2ubuntu1.2 [278 kB]\n",
            "Fetched 278 kB in 1s (368 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.3.3+dfsg-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking zstd (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "Setting up zstd (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "--2021-12-04 01:49:53--  https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.244\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.244|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9414712325 (8.8G) [application/octet-stream]\n",
            "Saving to: ‘step_383500_slim.tar.zstd’\n",
            "\n",
            "step_383500_slim.ta 100%[===================>]   8.77G  79.8MB/s    in 2m 6s   \n",
            "\n",
            "2021-12-04 01:51:59 (71.3 MB/s) - ‘step_383500_slim.tar.zstd’ saved [9414712325/9414712325]\n",
            "\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1334  100  1334    0     0   6539      0 --:--:-- --:--:-- --:--:--  6539\n",
            "--2021-12-04 01:53:33--  https://www.dropbox.com/s/ujqtabyxoos37f0/convert-n-run-query-loop.py\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ujqtabyxoos37f0/convert-n-run-query-loop.py [following]\n",
            "--2021-12-04 01:53:33--  https://www.dropbox.com/s/raw/ujqtabyxoos37f0/convert-n-run-query-loop.py\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc00a39db5207203c04c6f5cd7ee.dl.dropboxusercontent.com/cd/0/inline/BbI4NmPPCABhhQt5T-nNeKNmuQ6xlkU_rGB73SzOgqTK0C3k5fdbNyjC6U6mKW_1myCPgEuOc3TfZi61L5DLyDROHS7LhmyTby8GWREEsoyuzMj47zvuVEDRcCSeqYoGY3w8vLswV0y72sUIGvx0qmAb/file# [following]\n",
            "--2021-12-04 01:53:34--  https://uc00a39db5207203c04c6f5cd7ee.dl.dropboxusercontent.com/cd/0/inline/BbI4NmPPCABhhQt5T-nNeKNmuQ6xlkU_rGB73SzOgqTK0C3k5fdbNyjC6U6mKW_1myCPgEuOc3TfZi61L5DLyDROHS7LhmyTby8GWREEsoyuzMj47zvuVEDRcCSeqYoGY3w8vLswV0y72sUIGvx0qmAb/file\n",
            "Resolving uc00a39db5207203c04c6f5cd7ee.dl.dropboxusercontent.com (uc00a39db5207203c04c6f5cd7ee.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc00a39db5207203c04c6f5cd7ee.dl.dropboxusercontent.com (uc00a39db5207203c04c6f5cd7ee.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7290 (7.1K) [text/plain]\n",
            "Saving to: ‘convert-n-run-query-loop.py’\n",
            "\n",
            "convert-n-run-query 100%[===================>]   7.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-04 01:53:34 (669 MB/s) - ‘convert-n-run-query-loop.py’ saved [7290/7290]\n",
            "\n",
            "--2021-12-04 01:53:34--  https://www.dropbox.com/s/3vg5b8tjtvv16ag/query.py\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/3vg5b8tjtvv16ag/query.py [following]\n",
            "--2021-12-04 01:53:34--  https://www.dropbox.com/s/raw/3vg5b8tjtvv16ag/query.py\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdf871ccbaa52f32ac9a0918734.dl.dropboxusercontent.com/cd/0/inline/BbJPOyO1lkv80gHHvdPtotkquaQz3RC4rSHN2uqSSCH_T4uZE9g9mOUppBO-G_J1bMJ_AsawMsbdiUw0_bjGs1rFoiyNMXzvir39XXPNWeCG_0f_SxJsUDgWic_LIqnfoPAWi1iKAFhxZ6hB7WkWmXjD/file# [following]\n",
            "--2021-12-04 01:53:34--  https://ucdf871ccbaa52f32ac9a0918734.dl.dropboxusercontent.com/cd/0/inline/BbJPOyO1lkv80gHHvdPtotkquaQz3RC4rSHN2uqSSCH_T4uZE9g9mOUppBO-G_J1bMJ_AsawMsbdiUw0_bjGs1rFoiyNMXzvir39XXPNWeCG_0f_SxJsUDgWic_LIqnfoPAWi1iKAFhxZ6hB7WkWmXjD/file\n",
            "Resolving ucdf871ccbaa52f32ac9a0918734.dl.dropboxusercontent.com (ucdf871ccbaa52f32ac9a0918734.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucdf871ccbaa52f32ac9a0918734.dl.dropboxusercontent.com (ucdf871ccbaa52f32ac9a0918734.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3245 (3.2K) [text/plain]\n",
            "Saving to: ‘query.py’\n",
            "\n",
            "query.py            100%[===================>]   3.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-04 01:53:34 (749 MB/s) - ‘query.py’ saved [3245/3245]\n",
            "\n",
            "--2021-12-04 01:53:34--  https://www.dropbox.com/s/p0lgt11l6ssqkwl/run-query-loop.py\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/p0lgt11l6ssqkwl/run-query-loop.py [following]\n",
            "--2021-12-04 01:53:34--  https://www.dropbox.com/s/raw/p0lgt11l6ssqkwl/run-query-loop.py\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc33ba706a0eb90849a68d8dcd61.dl.dropboxusercontent.com/cd/0/inline/BbITkGjs6mHyhqiUUSF6QkZlHB3plKp1YnjXL2ATqmMiICIzpJj302lpmkiR63tMi2W11Swd_xM9F2W7l3wcHQb7m3W45Vu2ZJKYh5SVKAUTvhJkd_GAr8QZwJhfgVEcJloy-e-vtOi6QCjOQsZriRQw/file# [following]\n",
            "--2021-12-04 01:53:35--  https://uc33ba706a0eb90849a68d8dcd61.dl.dropboxusercontent.com/cd/0/inline/BbITkGjs6mHyhqiUUSF6QkZlHB3plKp1YnjXL2ATqmMiICIzpJj302lpmkiR63tMi2W11Swd_xM9F2W7l3wcHQb7m3W45Vu2ZJKYh5SVKAUTvhJkd_GAr8QZwJhfgVEcJloy-e-vtOi6QCjOQsZriRQw/file\n",
            "Resolving uc33ba706a0eb90849a68d8dcd61.dl.dropboxusercontent.com (uc33ba706a0eb90849a68d8dcd61.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc33ba706a0eb90849a68d8dcd61.dl.dropboxusercontent.com (uc33ba706a0eb90849a68d8dcd61.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1460 (1.4K) [text/plain]\n",
            "Saving to: ‘run-query-loop.py’\n",
            "\n",
            "run-query-loop.py   100%[===================>]   1.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-04 01:53:35 (201 MB/s) - ‘run-query-loop.py’ saved [1460/1460]\n",
            "\n",
            "loading shards for part 0\n",
            "read from checkpoint\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.wte.bias torch.Size([4096])\n",
            "< (8, 6300, 4096) to (1, 50400, 4096)\n",
            "> transformer.wte.weight torch.Size([4096, 50400])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "2021-12-04 01:53:57.755040: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "> transformer.h.0.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.0.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.0.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.0.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.0.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.0.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.0.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.0.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.0.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.0.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.1.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.1.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.1.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.1.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.1.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.1.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "loading shards for part 1\n",
            "read from checkpoint\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.1.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.1.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.1.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.1.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.10.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.10.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.10.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.10.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.10.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.10.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.10.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.10.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.10.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.10.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.11.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.11.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.11.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.11.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "loading shards for part 2\n",
            "read from checkpoint\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.11.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.11.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.11.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.11.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.11.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.11.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.12.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.12.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.12.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.12.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.12.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.12.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.12.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.12.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.12.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.12.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.13.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.13.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "loading shards for part 3\n",
            "read from checkpoint\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.13.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.13.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.13.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.13.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.13.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.13.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.13.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.13.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.14.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.14.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.14.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.14.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.14.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.14.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.14.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.14.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.14.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.14.ln_1.weight torch.Size([4096])\n",
            "loading shards for part 4\n",
            "read from checkpoint\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.15.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.15.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.15.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.15.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.15.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.15.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.15.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.15.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.15.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.15.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.16.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.16.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.16.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.16.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.16.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.16.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.16.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.16.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "loading shards for part 5\n",
            "read from checkpoint\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.16.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.16.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.17.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.17.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.17.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.17.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.17.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.17.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.17.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.17.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.17.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.17.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.18.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.18.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.18.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.18.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.18.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.18.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "loading shards for part 6\n",
            "read from checkpoint\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.18.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.18.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.18.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.18.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.19.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.19.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.19.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.19.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.19.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.19.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.19.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.19.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.19.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.19.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.2.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.2.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.2.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.2.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "loading shards for part 7\n",
            "read from checkpoint\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.2.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.2.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.2.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.2.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.2.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.2.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.20.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.20.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.20.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.20.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.20.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.20.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.20.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.20.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.20.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.20.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.21.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.21.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "loading shards for part 8\n",
            "read from checkpoint\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.21.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.21.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.21.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.21.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.21.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.21.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.21.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.21.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.22.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.22.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.22.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.22.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.22.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.22.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.22.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.22.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.22.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.22.ln_1.weight torch.Size([4096])\n",
            "loading shards for part 9\n",
            "read from checkpoint\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.23.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.23.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.23.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.23.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.23.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.23.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.23.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.23.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.23.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.23.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.24.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.24.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.24.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.24.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.24.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.24.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.24.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.24.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "loading shards for part 10\n",
            "read from checkpoint\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.24.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.24.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.25.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.25.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.25.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.25.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.25.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.25.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.25.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.25.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.25.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.25.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.26.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.26.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.26.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.26.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.26.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.26.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "loading shards for part 11\n",
            "read from checkpoint\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.26.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.26.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.26.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.26.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.27.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.27.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.27.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.27.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.27.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.27.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.27.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.27.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.27.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.27.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.3.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.3.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.3.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.3.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "loading shards for part 12\n",
            "read from checkpoint\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.3.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.3.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.3.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.3.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.3.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.3.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.4.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.4.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.4.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.4.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.4.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.4.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.4.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.4.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.4.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.4.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.5.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.5.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "loading shards for part 13\n",
            "read from checkpoint\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.5.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.5.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.5.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.5.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.5.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.5.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.5.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.5.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.6.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.6.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.6.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.6.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.6.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.6.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.6.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.6.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.6.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.6.ln_1.weight torch.Size([4096])\n",
            "loading shards for part 14\n",
            "read from checkpoint\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.7.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.7.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.7.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.7.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.7.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.7.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.7.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.7.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.7.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.7.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.8.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.8.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.8.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.8.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.8.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.8.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.8.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.8.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "loading shards for part 15\n",
            "read from checkpoint\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.8.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.8.ln_1.weight torch.Size([4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.9.attn.attention.q_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.9.attn.attention.v_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 4096, 512) to (1, 4096, 4096)\n",
            "> transformer.h.9.attn.attention.k_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 512, 4096) to (1, 4096, 4096)\n",
            "> transformer.h.9.attn.attention.out_proj.weight torch.Size([4096, 4096])\n",
            "< (8, 2048) to (1, 16384)\n",
            "> transformer.h.9.mlp.c_fc.bias torch.Size([16384])\n",
            "< (8, 4096, 2048) to (1, 4096, 16384)\n",
            "> transformer.h.9.mlp.c_fc.weight torch.Size([16384, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.9.mlp.c_proj.bias torch.Size([4096])\n",
            "< (8, 2048, 4096) to (1, 16384, 4096)\n",
            "> transformer.h.9.mlp.c_proj.weight torch.Size([4096, 16384])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.9.ln_1.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.h.9.ln_1.weight torch.Size([4096])\n",
            "< (8, 6300) to (1, 50400)\n",
            "> lm_head.bias torch.Size([50400])\n",
            "< (8, 4096, 6300) to (1, 4096, 50400)\n",
            "> lm_head.weight torch.Size([50400, 4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.ln_f.bias torch.Size([4096])\n",
            "< (8, 4096) to (4096,)\n",
            "> transformer.ln_f.weight torch.Size([4096])\n",
            "left over: [array([383502., 383502., 383502., 383502., 383502., 383502., 383502., 383502.], dtype=float32)]\n",
            "saving\n",
            "done\n",
            "convert-n-run-query-loop.py:172: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  class Checkpoint(collections.MutableMapping):\n",
            "load\n",
            "install_gpt-j.sh: line 24:  2132 Killed                  CUDA_VISIBLE_DEVICES=1 python convert-n-run-query-loop.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAQzTbqjV1yF"
      },
      "source": [
        "Parse the output prompt and response files. Sample 10 data point from each of the files and store them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLZled-Ld5GI"
      },
      "source": [
        "def parse_prompt_reponse(f):\n",
        "  prompt_response = []\n",
        "  while True:\n",
        "      next_line = f.readline()\n",
        "      if next_line == '':\n",
        "        break\n",
        "      elif next_line.startswith('Enter prompt or quit:'):\n",
        "        next_line = next_line[len('Enter prompt or quit:'):].strip(' ')\n",
        "      if next_line.startswith('Prompt:'):\n",
        "        prompt = next_line[len('Prompt:'):].strip()\n",
        "        next_line = f.readline()\n",
        "        response = []\n",
        "        while not next_line.startswith('Enter prompt or quit:'):\n",
        "          if next_line.startswith('Response:'):\n",
        "            response.append(next_line[len('Response:'):].strip())\n",
        "          else:\n",
        "            response.append(next_line)\n",
        "          next_line = f.readline()\n",
        "        prompt_response.append((prompt, ' '.join(response).strip()))\n",
        "  return prompt_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iou3kr6SaP5k",
        "outputId": "bf43db26-43c0-4db7-b64f-324b61905479"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# sample 10 data point from each of the files\n",
        "\n",
        "for f_name in os.listdir('/content/GPT-J/GPT-J-Run-Output'):\n",
        "  file_path = f'/content/GPT-J/GPT-J-Run-Output/{f_name}'\n",
        "  print(file_path)\n",
        "  f = open(file_path, 'r')\n",
        "  prompt_response = parse_prompt_reponse(f)\n",
        "  f.close()\n",
        "  prompt_response = np.array(prompt_response)\n",
        "  samples = prompt_response[np.random.choice(range(len(prompt_response)), 10)]\n",
        "  # sample 10 items\n",
        "  with open(f'{PROJECT_DIR}/results/{f_name}', 'w') as f:\n",
        "    for sample in samples:\n",
        "      prompt, response = sample\n",
        "      f.write('prompt: {}\\n'.format(prompt))\n",
        "      f.write('response: {}\\n'.format(response))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-J/GPT-J-Run-Output/drug_inhibits_output.txt\n",
            "/content/GPT-J/GPT-J-Run-Output/drug_targets_output.txt\n",
            "/content/GPT-J/GPT-J-Run-Output/drug_prompt_output.txt\n",
            "/content/GPT-J/GPT-J-Run-Output/names_aliases_output.txt\n",
            "/content/GPT-J/GPT-J-Run-Output/drug_mechanism_of_action_output.txt\n",
            "/content/GPT-J/GPT-J-Run-Output/names_patents_output.txt\n",
            "/content/GPT-J/GPT-J-Run-Output/gene_product_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Fx3deWWDa7"
      },
      "source": [
        "Sample 100 drugs and for each drug combine and save drug inhibition, mechanism and targers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50gK2YLw0Rl_",
        "outputId": "01c4501a-34f6-4a4c-99d2-f33d7b313dbd"
      },
      "source": [
        "import numpy as np\n",
        "# take 100 points each from 'drug_inhibits_output.txt', 'drug_mechanism_of_action_output.txt', 'drug_targets_output.txt'\n",
        "\n",
        "# select 100 drugs\n",
        "sample_indices = np.random.choice(range(2835), 100)\n",
        "file_path = f'/content/GPT-J/GPT-J-Run-Output/drug_inhibits_output.txt'\n",
        "f = open(file_path, 'r')\n",
        "prompt_response = parse_prompt_reponse(f)\n",
        "f.close()\n",
        "prompt_response = {prompt: reponse for (prompt, reponse) in prompt_response}\n",
        "\n",
        "drugs = prompt_response.keys()\n",
        "drugs = map(lambda x: x[:-len('is a drug that inhibits')], drugs)\n",
        "drugs = np.array(list(drugs))[sample_indices]\n",
        "print(drugs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['norethindrone' 'pimozide' 'Hyoscyamine' 'S4689' 'INCB28060'\n",
            " 'Oxtriphylline' 'L-Lactic' 'S5234' 'S5086' 'MK-2206' 'S5505'\n",
            " 'Molidustat(BAY' 'S4661' 'Fingolimod' 'S4668' 'Reboxetine' 'Ostarine'\n",
            " 'S5473' 'S4727' 'S3950' 'S5483' 'Prucalopride' 'Avanafil' 'Propranolol'\n",
            " 'S3650' 'S3969' 'LY3039478' 'Octanoic' 'S3761' 'Lumiracoxib' 'GSK1265744'\n",
            " 'Famciclovir' 'SAR125844' 'PENETREX' 'Ceftizoxime' 'S4870' 'Undecanoic'\n",
            " 'S3772' 'Ropivacaine' 'S3951' 'Tanshinone' 'Fumalic' 'LXS-196'\n",
            " 'Flumequine' 'TAK-117' 'Genistein' 'S9102' 'S5528' 'PD-0332991' 'S4059'\n",
            " 'MK-2206' 'RAPAMYCIN' 'S5592' 'Oxcarbazepine' 'LCI699' 'Diphenidol'\n",
            " 'Oltipraz' 'Oclacitinib' 'S5157' 'ALDOCLOR-250' 'S5629' 'L-NAME'\n",
            " 'Pentamidine' 'CETACORT' 'Pancuronium' 'BENOQUIN' 'S5627' 'Fulvestrant'\n",
            " 'Puerarin' 'S5581' 'S5302' 'GS-9620' 'S5398' 'Trospium' 'S4657'\n",
            " 'Demecarium' 'Tranexamic' 'estradiol' 'Brinzolamide' 'PLX-4032'\n",
            " 'nifuroxazide' 'AZOLID' 'Fluralaner' 'Oclacitinib' 'S5626' 'Estramustine'\n",
            " 'Rebamipide' 'Daidzein' 'S3745' 'S5119' 'KPT-8602' 'Parthenolide'\n",
            " 'Azilsartan' 'Fluticasone' 'S4602' 'Dalcetrapib' 'Ropinirole' 'S5601'\n",
            " 'Clomifene' 'Temsirolimus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW9iSezniwrs"
      },
      "source": [
        "from collections import defaultdict\n",
        "drug_fn_map = {\n",
        "    'drug_inhibits_output.txt': lambda x :  x.strip().split()[0][:-2], #S4819is a drug that inhibits\n",
        "    'drug_mechanism_of_action_output.txt': lambda x :  x.strip().split()[-2], # The mechanism of action of S4819 is\n",
        "    'drug_targets_output.txt': lambda x :  x.strip().split()[0] # S4637 is a drug that targets\n",
        "}\n",
        "\n",
        "\n",
        "drug_data = defaultdict(list)\n",
        "for f_name in ['drug_inhibits_output.txt', 'drug_mechanism_of_action_output.txt', 'drug_targets_output.txt']:\n",
        "  file_path = f'/content/GPT-J/GPT-J-Run-Output/{f_name}'\n",
        "  f = open(file_path, 'r')\n",
        "  prompt_response = parse_prompt_reponse(f)\n",
        "  f.close()\n",
        "prompt_response = {prompt: reponse for (prompt, reponse) in prompt_response}\n",
        "\n",
        "with open(f'{PROJECT_DIR}/results/drug_100.txt', 'w') as f:    \n",
        "  for drug in drug_data.keys():\n",
        "    inhibit, mechanism, target = drug_data[drug]\n",
        "    f.write(f'{drug}: \\n')\n",
        "    f.write(f'inhibits: {inhibit}\\n')\n",
        "    f.write(f'mechanism: {mechanism}\\n')\n",
        "    f.write(f'target: {target}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-duUmuJaX-ev"
      },
      "source": [
        "Create a bag of words model from the text in the 7 files. We will use a BPE tokenizer with 30,000 tokens as vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFlERTrLYRb6"
      },
      "source": [
        "from path import Path\n",
        "from sklearn import preprocessing\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_doMEgLvYjnF"
      },
      "source": [
        "def save_tokenizer(corpus_path, tokenizer_path):\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=30000)\n",
        "    tokenizer.train(files=list(Path(corpus_path).walkfiles('*.txt')), trainer=trainer)\n",
        "    tokenizer.save(tokenizer_path)\n",
        "    return\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    return Tokenizer.from_file(tokenizer_path)\n",
        "\n",
        "save_tokenizer('/content/GPT-J/GPT-J-Run-Output', '{}/tokenizer.txt'.format(PROJECT_DIR))\n",
        "tokenizer = load_tokenizer('{}/tokenizer.txt'.format(PROJECT_DIR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7syQE4OuboXy"
      },
      "source": [
        "def get_bow_vector(tokenizer, input):\n",
        "  ids = tokenizer.encode(input).ids \n",
        "  output = np.zeros((1, 30000))\n",
        "  for id in ids:\n",
        "    output[0][id] = 1\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdgb2Q48aS9m"
      },
      "source": [
        "tokenizer = load_tokenizer('{}/tokenizer.txt'.format(PROJECT_DIR))\n",
        "\n",
        "# gene products\n",
        "file_path = f'/content/GPT-J/GPT-J-Run-Output/gene_product_output.txt'\n",
        "f = open(file_path, 'r')\n",
        "gene_product = parse_prompt_reponse(f)\n",
        "f.close()\n",
        "\n",
        "gene_product = list(map(lambda x: (x[0].split()[3], x[1]), gene_product))\n",
        "gene_product_tokens = list(map(lambda x: (x[0], get_bow_vector(tokenizer, x[1])), gene_product))\n",
        "\n",
        "# drug targets\n",
        "file_path = f'/content/GPT-J/GPT-J-Run-Output/drug_targets_output.txt'\n",
        "f = open(file_path, 'r')\n",
        "drug_target = parse_prompt_reponse(f)\n",
        "f.close()\n",
        "\n",
        "drug_target = list(map(lambda x: (x[0].split()[0], x[1]), drug_target))\n",
        "drug_target_tokens = list(map(lambda x: (x[0], get_bow_vector(tokenizer, x[1])), drug_target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gmexb1Zvx5X"
      },
      "source": [
        "# patents\n",
        "file_path = f'/content/GPT-J/GPT-J-Run-Output/names_patents_output.txt'\n",
        "f = open(file_path, 'r')\n",
        "drug_patents = parse_prompt_reponse(f)\n",
        "f.close()\n",
        "\n",
        "drug_patents = list(map(lambda x: (x[0].split()[0], x[1]), drug_patents))\n",
        "drug_patents_tokens = list(map(lambda x: (x[0], get_bow_vector(tokenizer, x[1])), drug_patents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y_gZjYPggeC"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "gene_drug = []\n",
        "for i in range(min(100, len(gene_product))):\n",
        "  max_score, max_idx = 0, 0\n",
        "  for j in range(len(drug_target)):\n",
        "    similarity = 1 - spatial.distance.cosine(gene_product_tokens[i][1], drug_target_tokens[j][1])\n",
        "    if similarity > max_score:\n",
        "      max_score = similarity\n",
        "      max_idx = j\n",
        "  gene_drug.append((i,max_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdADJcXUwSy_"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "drug_patent = []\n",
        "for i in range(min(100, len(drug_target))):\n",
        "  max_score, max_idx = 0, 0\n",
        "  for j in range(len(drug_patents)):\n",
        "    similarity = 1 - spatial.distance.cosine(drug_target_tokens[i][1], drug_patents_tokens[j][1])\n",
        "    if similarity > max_score:\n",
        "      max_score = similarity\n",
        "      max_idx = j\n",
        "  drug_patent.append((i,max_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YN5JCCtlHnP"
      },
      "source": [
        "import re\n",
        "\n",
        "def get_gene_product(response):\n",
        "  pat = re.compile(r'(The gene product of[^\\.!?]*[\\.!?])', re.M)\n",
        "  return pat.findall(response)[0].split(maxsplit=7)[-1].strip(' .')\n",
        "\n",
        "# def get_drug_product(response):\n",
        "#   print(response)\n",
        "#   idx = response.find('is a drug that targets')\n",
        "#   return pat.findall(response)[:idx]\n",
        "\n",
        "gene_drug_prod = []\n",
        "for i in range(len(gene_drug)):\n",
        "  try:\n",
        "    drug = drug_target[gene_drug[i][1]][0]\n",
        "    gene = gene_product[gene_drug[i][0]][0]\n",
        "    prod = get_gene_product(gene_product[gene_drug[i][0]][1])\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    continue\n",
        "  gene_drug_prod.append((gene, prod, drug))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Yf9_0P8-p0vy",
        "outputId": "3afdec39-769a-4f51-cfa6-ff101fcc7089"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(gene_drug_prod, columns=['gene', 'product', 'drug'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gene</th>\n",
              "      <th>product</th>\n",
              "      <th>drug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>symbol</td>\n",
              "      <td>[</td>\n",
              "      <td>S5248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A1BG</td>\n",
              "      <td>glycoprotein</td>\n",
              "      <td>RVX-208(RVX-000222)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A1BG-AS1</td>\n",
              "      <td>antisense RNA that may regulate A1BG (alpha-1B...</td>\n",
              "      <td>CC-223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A1CF</td>\n",
              "      <td>novel cytidine deaminase, which is a member of...</td>\n",
              "      <td>Sulfamerazine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2M</td>\n",
              "      <td>which is a multifunctional,\\n bioactive, plasm...</td>\n",
              "      <td>NAQUIVAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>ABCD4</td>\n",
              "      <td>member of the family of ATP-binding cassette (...</td>\n",
              "      <td>PA-824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>ABCE1</td>\n",
              "      <td>membrane transporter protein of the ATP-bindin...</td>\n",
              "      <td>PA-824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>ABCF1</td>\n",
              "      <td>human ortholog of the yeast RAD1 gene, which e...</td>\n",
              "      <td>S5496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>ABCF1-DT</td>\n",
              "      <td>protein</td>\n",
              "      <td>RVX-208(RVX-000222)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>ABCF2</td>\n",
              "      <td>ATP-binding cassette (ABC) transporter</td>\n",
              "      <td>Radotinib</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        gene  ...                 drug\n",
              "0     symbol  ...                S5248\n",
              "1       A1BG  ...  RVX-208(RVX-000222)\n",
              "2   A1BG-AS1  ...               CC-223\n",
              "3       A1CF  ...        Sulfamerazine\n",
              "4        A2M  ...             NAQUIVAL\n",
              "..       ...  ...                  ...\n",
              "91     ABCD4  ...               PA-824\n",
              "92     ABCE1  ...               PA-824\n",
              "93     ABCF1  ...                S5496\n",
              "94  ABCF1-DT  ...  RVX-208(RVX-000222)\n",
              "95     ABCF2  ...            Radotinib\n",
              "\n",
              "[96 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSNebmOduk7T"
      },
      "source": [
        "import re\n",
        "\n",
        "def get_gene_product(response):\n",
        "  pat = re.compile(r'(The gene product of[^\\.!?]*[\\.!?])', re.M)\n",
        "  return pat.findall(response)[0].split(maxsplit=7)[-1].strip(' .')\n",
        "\n",
        "# def get_drug_product(response):\n",
        "#   print(response)\n",
        "#   idx = response.find('is a drug that targets')\n",
        "#   return pat.findall(response)[:idx]\n",
        "\n",
        "gene_drug_prod = []\n",
        "for i in range(len(gene_drug)):\n",
        "  try:\n",
        "    drug = drug_target[gene_drug[i][1]][0]\n",
        "    gene = gene_product[gene_drug[i][0]][0]\n",
        "    prod = get_gene_product(gene_product[gene_drug[i][0]][1])\n",
        "    patent = drug_patents[gene_drug[i][1]]\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    continue\n",
        "  gene_drug_prod.append((gene, prod, drug, patent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gPgT5H6mfgrA",
        "outputId": "d8ad79f2-a687-4834-afc8-0672860d3332"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(gene_drug_prod, columns=['gene', 'product', 'drug', 'patent'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gene</th>\n",
              "      <th>product</th>\n",
              "      <th>drug</th>\n",
              "      <th>patent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>symbol</td>\n",
              "      <td>[</td>\n",
              "      <td>S5248</td>\n",
              "      <td>(Cyproterone, Cyproterone is a drug mentioned ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A1BG</td>\n",
              "      <td>glycoprotein</td>\n",
              "      <td>RVX-208(RVX-000222)</td>\n",
              "      <td>(CARBAMAZEPINE, CARBAMAZEPINE is a drug mentio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A1BG-AS1</td>\n",
              "      <td>antisense RNA that may regulate A1BG (alpha-1B...</td>\n",
              "      <td>CC-223</td>\n",
              "      <td>(2-Pyridylacetonitrile, 2-Pyridylacetonitrile ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A1CF</td>\n",
              "      <td>novel cytidine deaminase, which is a member of...</td>\n",
              "      <td>Sulfamerazine</td>\n",
              "      <td>(Fluralaner, Fluralaner is a drug mentioned in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2M</td>\n",
              "      <td>which is a multifunctional,\\n bioactive, plasm...</td>\n",
              "      <td>NAQUIVAL</td>\n",
              "      <td>(Ammonium, Ammonium is a drug mentioned in pat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>ABCD1P4</td>\n",
              "      <td>ATP-binding cassette transporter protein that ...</td>\n",
              "      <td>Macitentan</td>\n",
              "      <td>(AVC, AVC is a drug mentioned in patent litera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>ABCD4</td>\n",
              "      <td>member of the family of ATP-binding cassette (...</td>\n",
              "      <td>S5630</td>\n",
              "      <td>(EAI045, EAI045 is a drug mentioned in patent ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>ABCE1</td>\n",
              "      <td>membrane transporter protein of the ATP-bindin...</td>\n",
              "      <td>S3892</td>\n",
              "      <td>(CL-387785, CL-387785 is a drug mentioned in p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>ABCF1</td>\n",
              "      <td>human ortholog of the yeast RAD1 gene, which e...</td>\n",
              "      <td>MK-2206</td>\n",
              "      <td>(Alfacalcidol, Alfacalcidol is a drug mentione...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>ABCF2</td>\n",
              "      <td>ATP-binding cassette (ABC) transporter</td>\n",
              "      <td>S3769</td>\n",
              "      <td>(CL-278474, CL-278474 is a drug mentioned in p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>152 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         gene  ...                                             patent\n",
              "0      symbol  ...  (Cyproterone, Cyproterone is a drug mentioned ...\n",
              "1        A1BG  ...  (CARBAMAZEPINE, CARBAMAZEPINE is a drug mentio...\n",
              "2    A1BG-AS1  ...  (2-Pyridylacetonitrile, 2-Pyridylacetonitrile ...\n",
              "3        A1CF  ...  (Fluralaner, Fluralaner is a drug mentioned in...\n",
              "4         A2M  ...  (Ammonium, Ammonium is a drug mentioned in pat...\n",
              "..        ...  ...                                                ...\n",
              "147   ABCD1P4  ...  (AVC, AVC is a drug mentioned in patent litera...\n",
              "148     ABCD4  ...  (EAI045, EAI045 is a drug mentioned in patent ...\n",
              "149     ABCE1  ...  (CL-387785, CL-387785 is a drug mentioned in p...\n",
              "150     ABCF1  ...  (Alfacalcidol, Alfacalcidol is a drug mentione...\n",
              "151     ABCF2  ...  (CL-278474, CL-278474 is a drug mentioned in p...\n",
              "\n",
              "[152 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFO3SyiZS7ur"
      },
      "source": [
        "# Writeup\n",
        "\n",
        "I sampled 10 prompt and responses from each of the 7 files each. I tried to verify the truth of responses. I could not verify most of the responses. It looked like the response was completedly made using similar meaning words from different references. \n",
        "\n"
      ]
    }
  ]
}